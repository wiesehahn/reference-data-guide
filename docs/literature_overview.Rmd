---
output: github_document
bibliography: ["../literature.bib"]
link-citations: true
---
# Reference Data for Random Forest
## A Guide for Tree Species Classification with Multispectral Satellite Imagery

Often, one set of reference data is used for training and validation. Whereas splitting the data set in a training set and a validation set is a valid option as long as they are independent (?), using the same data for both does not result in an unbiased accuracy estimation. But also splitting a data set and using one part for the prediction and and the other part for the validation is in many cases not the best option as different requirements exist. Training data requirements are dependent on the classification algorithm and do not necessarily need to be uncorrelated. For an unbiased estimation of classification accuracy the validation data on the other hand needs to follow a probability sampling scheme.

## Training data 

### Data set size

How many training data samples should be used?

- as many as possible: [@millardImportanceTrainingData2015]
- not sensitive to reduction: [@rodriguez-galianoAssessmentEffectivenessRandom2012]

How to distribute the training data among classes?

- area proportional: [@colditzEvaluationDifferentTraining2015; @freemanEvaluatingEffectivenessDownsampling2012; @millardImportanceTrainingData2015]
- balanced is good but fine imbalance (best/worst class) is better [@mellorExploringIssuesTraining2015] 

If data has to be resampled what method should be used?

- upsampling: [@japkowiczClassImbalanceProblem2002; @mccarthyDoesCostsensitiveLearning2005]

Which other methods are suggested to consider class imbalance?

- threshold optimization: [@freemanEvaluatingEffectivenessDownsampling2012]
- balanced random forest: [@chenUsingRandomForest2004]
- weighted random forest: [@chenUsingRandomForest2004]
- cost sensitive learning: [@japkowiczClassImbalanceProblem2002; @mccarthyDoesCostsensitiveLearning2005]

Is it better to use homogeneous or heterogeneous data within classes?

- heterogeneous: [@colditzEvaluationDifferentTraining2015]

Is it better to use more or less predictor variables?

- as few as necessary: [@millardImportanceTrainingData2015]





## Key Statements

@rodriguez-galianoAssessmentEffectivenessRandom2012

- "RF has low sensitivity to the training set size reduction"
- "RF is relatively little noise sensitive"
- used equal number of samples per class
- as RF generates unbiased estimation of error (oob) a validation data set is unnecessary


@colditzEvaluationDifferentTraining2015

- "only a few studies have focused on training data allocation schemes, such as between-class sample balance or the structure of heterogeneous samples"
- "classification trees may suffer from an unbalanced sample size between classes because the number of samples in each leaf defines the class"
- "A few studies recommend heterogeneous training data for discrete classification but most large-area mapping projects select homogeneous areas for training"
- validated with equal number of samples per class
- "For classification trees, heterogeneous training pixels are recommended"
- "Area-proportional allocation is recommended"


@freemanEvaluatingEffectivenessDownsampling2012

- "balancing the number of presences and absences in a training data set by down-sampling did not improve predictive models"
- "By seeking to minimize overall error, such algorithms can increase the error rate for the rare
class"
- "in theory, Balanced models are proposed as a way of improving predictions in species with very low or very high prevalence. Instead we found that when the threshold is kept at the default of 0.5, Balanced models actually did worse than the Baseline models for our very low prevalence species"
- "the default Baseline model tended to over predict the high prevalence species and under predict the low prevalence species, while the default Balanced model over predicted all but one of our species and highly over predicted the rare species"
- "the predictive performance in terms of AUC, kappa and prevalence of the Balanced model was
considerably improved by threshold optimization"


@chenUsingRandomForest2004a

- "similar to most classifiers, RF can also suffer from the curse of learning from an extremely imbalanced training data set. As it is constructed to minimize the overall error rate, it will tend to focus more on the prediction accuracy of the majority class, which often results in poor accuracy for the minority class"

- "Both methods  [Balanced RF and Weighted RF] are shown to improve the prediction accuracy of
the minority class, and have favorable performance compared to the existing algorithms"


@mccarthyDoesCostsensitiveLearning2005

- "between cost-sensitive learning, up-sampling, and down-sampling, there is no clear or
consistent winner for maximizing classifier performance"
- for small data sets "up-sampling clearly does much better than down-sampling and cost-sensitive learning"
- "for the large data sets, cost-sensitive learning does often yield the best results"


@japkowiczClassImbalanceProblem2002

- linearly separable (not complex) data is not sensitive to imbalance
- sensitivity to imbalance increases with complexity level
- class imbalance is not a problem if training set size is large
- "the higher the degree of class imbalance the higher the complexity of the concept and the smaller the overall size of the training set, the greate the effect of class imbalances" 
- all methods (under-, oversampling, cost-modifying) improve classification
- under-sampling is least effective (in case where no data is irrelevant e.g. when majority class consists of repeated samples)
- over-sampling is quite effective in these cases
- cost-modifying is more effective than resampling


@mellorExploringIssuesTraining2015

- "the error rate of the RF classifier is relatively insensitive to mislabelled training data"
- "balanced training data resulted in the lowest overall error rates for classification experiments"
- "imbalance can be introduced to improve error rates of more difficult classes, without adversely affecting overall classification accuracy"


@millardImportanceTrainingData2015

- "classifiers may be biased where the proportions of training or validation data classes are distributed unequally or are imbalanced relative to the actual land cover proportions"
- "increasing sample size significantly should lead to improved classification accuracy"
- "Once the proportion of the class of interest was increased to near its actual proportion in the landscape, that class always became the class with the lowest proportion-error. As its proportion increased beyond its actual proportion in the landscape, rfOOB error tended towards zero and the
proportion-error for that class increased"
- "As the proportion of the class of interest in the training data set increased, the resulting proportion of that class in the predicted image also increased"
- "As spatial autocorrelation of training data increased, rfOOB error decreased while independent assessment error increased"
- "RF image classification is highly sensitive to training data characteristics, including sample size, class proportions and spatial autocorrelation"
- "larger training sample sizes are recommended to improve classification accuracy"
- "variable reduction should be performed to obtain the optimum classification. When high dimensional datasets were used, classification results were noisy"
- "classification results reflected the forced proportions"
- "These results indicate the importance of carefully selecting training data sample points without bias and so that landscape proportions are maintained"
- "High-dimension datasets should be reduced. Using only important, uncorrelated variables"
- "As many training and validation sample points should be collected as possible"
- "An unbiased sampling strategy that ensures representative class proportions should be used"
- "spatial autocorrelation should be minimized within the training and validation data sample points"




## Schemes used in selected studies

@wesselEvaluationDifferentMachine2018

- equal number of samples per class 
- from inventory data
- only pure stands
- multiple samples per area


@perssonTreeSpeciesClassification2018

- from systematic grid and subjective plots
- only dominated stands (>70% BA)
- samples for rare classes were added (sampling scheme between equal and proportional)
- reference used for training and testing (with cross-validation)


@limMachineLearningTree2020

- equal number of samples per class 
- only sparse information about the validation data and process


@hosciloMappingForestType2019

- (almost) equal number of samples per class
- from inventory data and randomly added plots
- only dominated stands


@grabskaForestStandSpecies2019

- more or less proportional number of samples per class
- from inventory data 
- only pure (main species) or dominant (rare species) stands were selected (and spectrally homogeneous)
- all pixels within polygon per area 


@bjerreskovForestTypeTree2021

- from inventory data
- validation data from different plots
- only dominant stands
- more or less proportional reference data (train and validation)


@kollertExploringPotentialLand2021

- reference data from subjective polygons
- only pure stands but some species merged in one class
- seperation between train and validation polygons (spatially uncorrelated between but correlated within reference sets)
- folded cross-validation used
- unequal number of reference samples but not proportionally


@breidenbachNationalMappingEstimation2020

- training data from NFI plots (proportional)
- for all unsplit forest plots the dominant species was determined
- validation data from independent stand inventories
- validation data for pure stands (>95%)
- models were validated using cross-validation with training data
- models were validated with independent forest stands



